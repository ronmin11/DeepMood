<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>DeepMood</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- Tailwind CSS via CDN -->
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-gray-100 min-h-screen">
  <div class="max-w-3xl mx-auto py-10 space-y-16">
    <!-- Section 1: Title -->
    <section class="text-center">
      <h1 class="text-5xl font-extrabold text-blue-700 mb-2">DeepMood</h1>
    </section>

    <!-- Section 2: Description -->
    <section class="bg-white rounded-lg shadow p-6">
      <h2 class="text-2xl font-bold mb-4 text-gray-800">About DeepMood</h2>
      <p class="text-gray-700 text-lg">
        This novel implementation aims to address the lack of empathy and emotional awareness in everyday interactions. When individuals struggle to recognize or understand others' emotions, it can contribute to feelings of isolation, loneliness, or depression in those who go unnoticed or misunderstood. A potential remedy for this issue would be to utilize machine-learning-based emotion classifiers with CNN architectures and fully connected layers to efficiently and effectively determine the emotional state of an individual given a live feed from glasses. This could develop into more practical uses, such as in real-world hospitals for improved psychological therapy diagnosis.
      </p>
    </section>

    <!-- Section 3: AI Model Application (Face Tracking) -->
    <section class="bg-white rounded-lg shadow p-6">
      <h2 class="text-2xl font-bold mb-4 text-gray-800">AI Face Tracking Demo</h2>
      <p class="mb-4 text-gray-600">Allow camera access to see real-time face tracking in action.</p>
      <div class="flex flex-col items-center w-[480px] h-[360px] mx-auto relative">
        <video id="video" width="480" height="360" autoplay muted playsinline class="rounded border-2 border-blue-400 bg-black z-10 block"></video>
        <canvas id="overlay" width="480" height="360" class="absolute left-0 top-0 z-20 pointer-events-none"></canvas>
        <div id="video-fallback" class="absolute left-0 top-0 w-full h-full flex items-center justify-center bg-black bg-opacity-70 text-white text-lg font-bold hidden">Camera not available</div>
      </div>
      <p id="face-status" class="mt-4 text-blue-600 font-semibold"></p>
      <p id="camera-error" class="mt-2 text-red-600 font-semibold hidden"></p>
      <div class="mt-2">
        <span class="font-semibold">Predicted Emotion:</span> <span id="emotion-result" class="text-pink-600 font-bold">-</span>
        <span class="ml-4 font-semibold">Confidence:</span> <span id="confidence-result" class="text-green-600 font-bold">-</span>
      </div>
      <!-- Chatbot UI -->
      <div class="mt-8 block">
        <h3 class="text-xl font-bold mb-2 text-gray-700">Therapist Chatbot</h3>
        <div id="chat-window" class="bg-gray-50 border rounded p-4 h-64 overflow-y-auto mb-2 text-left"></div>
        <form id="chat-form" class="flex gap-2">
          <input id="chat-input" type="text" class="flex-1 border rounded px-3 py-2" placeholder="Type your message..." required />
          <button type="submit" class="bg-blue-600 hover:bg-blue-700 text-white font-semibold px-4 py-2 rounded">Send</button>
        </form>
      </div>
      <!-- Image Upload for Emotion Prediction -->
      <div class="mt-8 block">
        <h3 class="text-xl font-bold mb-2 text-gray-700">Upload Image for Emotion Prediction</h3>
        <form id="upload-form" class="flex flex-col items-center gap-4">
          <input id="img-upload" type="file" accept="image/*" class="border rounded px-3 py-2" required />
          <img id="img-preview" src="" alt="Preview" class="max-w-xs rounded shadow hidden" />
          <button type="submit" class="bg-green-600 hover:bg-green-700 text-white font-semibold px-4 py-2 rounded">Predict Emotion</button>
        </form>
        <div class="mt-2">
          <span class="font-semibold">Predicted Emotion:</span> <span id="upload-emotion-result" class="text-pink-600 font-bold">-</span>
          <span class="ml-4 font-semibold">Confidence:</span> <span id="upload-confidence-result" class="text-green-600 font-bold">-</span>
        </div>
      </div>
    </section>

    <!-- Section 4: Contact -->
    <section class="bg-white rounded-lg shadow p-6">
      <h2 class="text-2xl font-bold mb-4 text-gray-800">Contact</h2>
      <form class="space-y-4">
        <div>
          <label class="block text-gray-700">Name</label>
          <input type="text" class="w-full border rounded px-3 py-2" required />
        </div>
        <div>
          <label class="block text-gray-700">Email</label>
          <input type="email" class="w-full border rounded px-3 py-2" required />
        </div>
        <div>
          <label class="block text-gray-700">Message</label>
          <textarea class="w-full border rounded px-3 py-2" rows="4" required></textarea>
        </div>
        <button type="submit" class="bg-blue-600 hover:bg-blue-700 text-white font-semibold py-2 px-6 rounded transition">Send</button>
      </form>
    </section>
  </div>

  <script>
    // Face tracking using face-api.js
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const faceStatus = document.getElementById('face-status');
    let displaySize = { width: 480, height: 360 };

    async function setupCamera() {
      const cameraError = document.getElementById('camera-error');
      const videoFallback = document.getElementById('video-fallback');
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        video.onloadedmetadata = () => {
          video.play();
          cameraError.classList.add('hidden');
          cameraError.textContent = '';
          videoFallback.classList.add('hidden');
          console.log('Camera stream started.');
        };
        // Fallback: if video doesn't start in 5 seconds, show fallback
        setTimeout(() => {
          if (video.readyState < 2) {
            videoFallback.classList.remove('hidden');
            cameraError.textContent = 'Camera stream could not be started. Please check your camera and browser permissions.';
            cameraError.classList.remove('hidden');
            console.error('Camera stream did not start in time.');
          }
        }, 5000);
      } catch (err) {
        faceStatus.innerText = 'Camera access denied or not available.';
        cameraError.textContent = 'Camera access denied or not available. Please check your browser settings and allow camera access.';
        cameraError.classList.remove('hidden');
        videoFallback.classList.remove('hidden');
        console.error('Camera error:', err);
      }
    }

    async function loadModels() {
      faceStatus.innerText = 'Loading face detection models...';
      try {
        await faceapi.nets.tinyFaceDetector.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models');
        await faceapi.nets.faceLandmark68TinyNet.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models');
        faceStatus.innerText = 'Models loaded. Initializing camera...';
        await setupCamera();
        faceStatus.innerText = 'Camera ready. Detecting faces...';
        runFaceDetection();
      } catch (err) {
        faceStatus.innerText = 'Failed to load face detection models.';
        console.error('Model load error:', err);
      }
    }

    async function runFaceDetection() {
      const canvas = overlay;
      faceapi.matchDimensions(canvas, displaySize);
      let lastSent = 0;
      setInterval(async () => {
        if (video.readyState === 4) {
          const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks(true);
          const resizedDetections = faceapi.resizeResults(detections, displaySize);
          canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
          faceapi.draw.drawDetections(canvas, resizedDetections);
          faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
          faceStatus.innerText = detections.length > 0 ? `Faces detected: ${detections.length}` : 'No face detected.';

          // If a face is detected, send frame every 2 seconds
          if (detections.length > 0 && Date.now() - lastSent > 2000) {
            sendFrameToBackend();
            lastSent = Date.now();
          }
        }
      }, 200);
    }

    // Send current video frame to backend for emotion prediction
    function sendFrameToBackend() {
      const emotionResult = document.getElementById('emotion-result');
      const confidenceResult = document.getElementById('confidence-result');
      try {
        // Create a temporary canvas to capture the frame
        const tempCanvas = document.createElement('canvas');
        tempCanvas.width = video.videoWidth;
        tempCanvas.height = video.videoHeight;
        const ctx = tempCanvas.getContext('2d');
        ctx.drawImage(video, 0, 0, tempCanvas.width, tempCanvas.height);
        tempCanvas.toBlob(async (blob) => {
          if (!blob) return;
          const formData = new FormData();
          formData.append('image', blob, 'frame.jpg');
          try {
            const response = await fetch('http://localhost:5000/predict', {
              method: 'POST',
              body: formData
            });
            if (response.ok) {
              const data = await response.json();
              emotionResult.textContent = data.emotion || '-';
              if (typeof data.confidence === 'number') {
                confidenceResult.textContent = (data.confidence * 100).toFixed(2) + '%';
              } else {
                confidenceResult.textContent = '-';
              }
            } else {
              emotionResult.textContent = '-';
              confidenceResult.textContent = '-';
            }
          } catch (err) {
            emotionResult.textContent = '-';
            confidenceResult.textContent = '-';
          }
        }, 'image/jpeg');
      } catch (err) {
        emotionResult.textContent = '-';
        confidenceResult.textContent = '-';
      }
    }

    // Chatbot logic
    const chatForm = document.getElementById('chat-form');
    const chatInput = document.getElementById('chat-input');
    const chatWindow = document.getElementById('chat-window');

    chatForm.addEventListener('submit', async (e) => {
      e.preventDefault();
      const userMsg = chatInput.value.trim();
      if (!userMsg) return;
      appendMessage('You', userMsg);
      chatInput.value = '';
      chatInput.disabled = true;
      try {
        const res = await fetch('/chatbot', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ message: userMsg })
        });
        if (res.ok) {
          const data = await res.json();
          appendMessage('Therapist', data.reply || '(No response)');
        } else {
          appendMessage('Therapist', '(Error: No response)');
          console.error('Chatbot fetch error:', res.statusText);
        }
      } catch (err) {
        appendMessage('Therapist', '(Network error)');
        console.error('Chatbot network error:', err);
      }
      chatInput.disabled = false;
      chatInput.focus();
    });

    function appendMessage(sender, text) {
      const msgDiv = document.createElement('div');
      msgDiv.innerHTML = `<span class="font-bold">${sender}:</span> <span>${escapeHtml(text)}</span>`;
      chatWindow.appendChild(msgDiv);
      chatWindow.scrollTop = chatWindow.scrollHeight;
    }

    function escapeHtml(text) {
      return text.replace(/[&<>"]/g, function(c) {
        return {'&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;'}[c];
      });
    }

    // Image upload logic
    const uploadForm = document.getElementById('upload-form');
    const imgUpload = document.getElementById('img-upload');
    const imgPreview = document.getElementById('img-preview');
    const uploadEmotionResult = document.getElementById('upload-emotion-result');
    const uploadConfidenceResult = document.getElementById('upload-confidence-result');

    imgUpload.addEventListener('change', () => {
      const file = imgUpload.files[0];
      if (file) {
        const reader = new FileReader();
        reader.onload = (e) => {
          imgPreview.src = e.target.result;
          imgPreview.classList.remove('hidden');
        };
        reader.readAsDataURL(file);
      } else {
        imgPreview.src = '';
        imgPreview.classList.add('hidden');
      }
    });

    uploadForm.addEventListener('submit', async (e) => {
      e.preventDefault();
      const file = imgUpload.files[0];
      if (!file) return;
      uploadEmotionResult.textContent = '...';
      uploadConfidenceResult.textContent = '...';
      const formData = new FormData();
      formData.append('image', file);
      try {
        const response = await fetch('http://localhost:5000/predict', {
          method: 'POST',
          body: formData
        });
        if (response.ok) {
          const data = await response.json();
          uploadEmotionResult.textContent = data.emotion || '-';
          if (typeof data.confidence === 'number') {
            uploadConfidenceResult.textContent = (data.confidence * 100).toFixed(2) + '%';
          } else {
            uploadConfidenceResult.textContent = '-';
          }
        } else {
          uploadEmotionResult.textContent = '-';
          uploadConfidenceResult.textContent = '-';
        }
      } catch (err) {
        uploadEmotionResult.textContent = '-';
        uploadConfidenceResult.textContent = '-';
      }
    });

    window.addEventListener('DOMContentLoaded', loadModels);
  </script>
  <!-- face-api.js via CDN (must be loaded after DOM and before use) -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
</body>
</html>
