<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>DeepMood</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- Tailwind CSS via CDN -->
  <script src="https://cdn.tailwindcss.com"></script>
  <!-- face-api.js via CDN -->
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
</head>
<body class="bg-gray-100 min-h-screen">
  <div class="max-w-3xl mx-auto py-10 space-y-16">
    <!-- Section 1: Title -->
    <section class="text-center">
      <h1 class="text-5xl font-extrabold text-blue-700 mb-2">DeepMood</h1>
    </section>

    <!-- Section 2: Description -->
    <section class="bg-white rounded-lg shadow p-6">
      <h2 class="text-2xl font-bold mb-4 text-gray-800">About DeepMood</h2>
      <p class="text-gray-700 text-lg">
        This novel implementation aims to address the lack of empathy and emotional awareness in everyday interactions. When individuals struggle to recognize or understand others' emotions, it can contribute to feelings of isolation, loneliness, or depression in those who go unnoticed or misunderstood. A potential remedy for this issue would be to utilize machine-learning-based emotion classifiers with CNN architectures and fully connected layers to efficiently and effectively determine the emotional state of an individual given a live feed from glasses. This could develop into more practical uses, such as in real-world hospitals for improved psychological therapy diagnosis.
      </p>
    </section>

    <!-- Section 3: AI Model Application (Face Tracking) -->
    <section class="bg-white rounded-lg shadow p-6">
      <h2 class="text-2xl font-bold mb-4 text-gray-800">AI Face Tracking Demo</h2>
      <p class="mb-4 text-gray-600">Allow camera access to see real-time face tracking in action.</p>
      <div class="flex flex-col items-center w-[480px] h-[360px] mx-auto relative">
        <video id="video" width="480" height="360" autoplay muted playsinline class="rounded border bg-black z-10 block"></video>
        <canvas id="overlay" width="480" height="360" class="absolute left-0 top-0 z-20 pointer-events-none"></canvas>
      </div>
      <p id="face-status" class="mt-4 text-blue-600 font-semibold"></p>
      <p id="camera-error" class="mt-2 text-red-600 font-semibold hidden"></p>
      <div class="mt-2">
        <span class="font-semibold">Predicted Emotion:</span> <span id="emotion-result" class="text-pink-600 font-bold">-</span>
      </div>
      <!-- Chatbot UI -->
      <div class="mt-8 block">
        <h3 class="text-xl font-bold mb-2 text-gray-700">Therapist Chatbot</h3>
        <div id="chat-window" class="bg-gray-50 border rounded p-4 h-64 overflow-y-auto mb-2 text-left"></div>
        <form id="chat-form" class="flex gap-2">
          <input id="chat-input" type="text" class="flex-1 border rounded px-3 py-2" placeholder="Type your message..." required />
          <button type="submit" class="bg-blue-600 hover:bg-blue-700 text-white font-semibold px-4 py-2 rounded">Send</button>
        </form>
      </div>
    </section>

    <!-- Section 4: Contact -->
    <section class="bg-white rounded-lg shadow p-6">
      <h2 class="text-2xl font-bold mb-4 text-gray-800">Contact</h2>
      <form class="space-y-4">
        <div>
          <label class="block text-gray-700">Name</label>
          <input type="text" class="w-full border rounded px-3 py-2" required />
        </div>
        <div>
          <label class="block text-gray-700">Email</label>
          <input type="email" class="w-full border rounded px-3 py-2" required />
        </div>
        <div>
          <label class="block text-gray-700">Message</label>
          <textarea class="w-full border rounded px-3 py-2" rows="4" required></textarea>
        </div>
        <button type="submit" class="bg-blue-600 hover:bg-blue-700 text-white font-semibold py-2 px-6 rounded transition">Send</button>
      </form>
    </section>
  </div>

  <script>
    // Face tracking using face-api.js
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const faceStatus = document.getElementById('face-status');
    let displaySize = { width: 480, height: 360 };

    async function setupCamera() {
      const cameraError = document.getElementById('camera-error');
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        video.onloadedmetadata = () => {
          video.play();
          cameraError.classList.add('hidden');
          cameraError.textContent = '';
          console.log('Camera stream started.');
        };
      } catch (err) {
        faceStatus.innerText = 'Camera access denied or not available.';
        cameraError.textContent = 'Camera access denied or not available. Please check your browser settings and allow camera access.';
        cameraError.classList.remove('hidden');
        console.error('Camera error:', err);
      }
    }

    async function loadModels() {
      faceStatus.innerText = 'Loading face detection models...';
      try {
        await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/models');
        await faceapi.nets.faceLandmark68TinyNet.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js/models');
        faceStatus.innerText = 'Models loaded. Initializing camera...';
        await setupCamera();
        faceStatus.innerText = 'Camera ready. Detecting faces...';
        runFaceDetection();
      } catch (err) {
        faceStatus.innerText = 'Failed to load face detection models.';
        console.error('Model load error:', err);
      }
    }

    async function runFaceDetection() {
      const canvas = overlay;
      faceapi.matchDimensions(canvas, displaySize);
      let lastSent = 0;
      setInterval(async () => {
        if (video.readyState === 4) {
          const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks(true);
          const resizedDetections = faceapi.resizeResults(detections, displaySize);
          canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
          faceapi.draw.drawDetections(canvas, resizedDetections);
          faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
          faceStatus.innerText = detections.length > 0 ? `Faces detected: ${detections.length}` : 'No face detected.';

          // If a face is detected, send frame every 2 seconds
          if (detections.length > 0 && Date.now() - lastSent > 2000) {
            sendFrameToBackend();
            lastSent = Date.now();
          }
        }
      }, 200);
    }

    // Send current video frame to backend for emotion prediction
    function sendFrameToBackend() {
      const emotionResult = document.getElementById('emotion-result');
      try {
        // Create a temporary canvas to capture the frame
        const tempCanvas = document.createElement('canvas');
        tempCanvas.width = video.videoWidth;
        tempCanvas.height = video.videoHeight;
        const ctx = tempCanvas.getContext('2d');
        ctx.drawImage(video, 0, 0, tempCanvas.width, tempCanvas.height);
        tempCanvas.toBlob(async (blob) => {
          if (!blob) return;
          const formData = new FormData();
          formData.append('image', blob, 'frame.jpg');
          try {
            const response = await fetch('/predict', {
              method: 'POST',
              body: formData
            });
            if (response.ok) {
              const data = await response.json();
              emotionResult.textContent = data.emotion || '-';
            } else {
              emotionResult.textContent = '-';
            }
          } catch (err) {
            emotionResult.textContent = '-';
          }
        }, 'image/jpeg');
      } catch (err) {
        emotionResult.textContent = '-';
      }
    }

    // Chatbot logic
    const chatForm = document.getElementById('chat-form');
    const chatInput = document.getElementById('chat-input');
    const chatWindow = document.getElementById('chat-window');

    chatForm.addEventListener('submit', async (e) => {
      e.preventDefault();
      const userMsg = chatInput.value.trim();
      if (!userMsg) return;
      appendMessage('You', userMsg);
      chatInput.value = '';
      chatInput.disabled = true;
      try {
        const res = await fetch('/chatbot', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ message: userMsg })
        });
        if (res.ok) {
          const data = await res.json();
          appendMessage('Therapist', data.reply || '(No response)');
        } else {
          appendMessage('Therapist', '(Error: No response)');
          console.error('Chatbot fetch error:', res.statusText);
        }
      } catch (err) {
        appendMessage('Therapist', '(Network error)');
        console.error('Chatbot network error:', err);
      }
      chatInput.disabled = false;
      chatInput.focus();
    });

    function appendMessage(sender, text) {
      const msgDiv = document.createElement('div');
      msgDiv.innerHTML = `<span class="font-bold">${sender}:</span> <span>${escapeHtml(text)}</span>`;
      chatWindow.appendChild(msgDiv);
      chatWindow.scrollTop = chatWindow.scrollHeight;
    }

    function escapeHtml(text) {
      return text.replace(/[&<>"]/g, function(c) {
        return {'&':'&amp;','<':'&lt;','>':'&gt;','"':'&quot;'}[c];
      });
    }

    window.addEventListener('DOMContentLoaded', loadModels);
  </script>
</body>
</html>
